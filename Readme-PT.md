ğŸš€ Projeto-IA-Local
Este Ã© um ambiente de IA local alimentado pelo Ollama, Docker e Open-WebUI. Ele permite executar e interagir com modelos de IA (como LLaMA 3) diretamente na sua mÃ¡quina atravÃ©s de uma interface similar ao ChatGPTâ€”privado, rÃ¡pido e personalizÃ¡vel em apenas 6 passos!
ğŸ¯ O Que VocÃª ObtÃ©m
ğŸ›¡ï¸ IA Focada na Privacidade: Mantenha seus dados locaisâ€”sem necessidade de nuvem.
â†’ Suas conversas e dados ficam na sua mÃ¡quina, garantindo controle total e privacidade.
ğŸ§  Flexibilidade de Modelos: Execute e alterne entre modelos como LLaMA 2, Mistral, ou qualquer modelo suportado pelo Ollama.
â†’ O Ollama suporta uma variedade de modelos de cÃ³digo abertoâ€”escolha o que melhor se adequa Ã s suas necessidades.
ğŸ§© PersonalizaÃ§Ã£o: Use o Open-WebUI para dar memÃ³ria e personalidade Ã  sua IA.
â†’ Personalize como sua IA se comporta e interage com vocÃª.
ğŸ’¾ Armazenamento Persistente: Os volumes do Docker mantÃªm seus modelos e dados segurosâ€”mesmo apÃ³s uma reinicializaÃ§Ã£o.
â†’ Modelos baixados e configuraÃ§Ãµes nÃ£o desaparecerÃ£o quando o container parar ou reiniciar.
ğŸŒ Compartilhamento Online Opcional: Use ferramentas como ngrok para compartilhar sua IA com outros de forma segura pela internet.
â†’ Ã“timo para acesso remoto, demonstraÃ§Ãµes, ou colaboraÃ§Ã£o com amigos e equipes.
ğŸ› ï¸ Passos de InstalaÃ§Ã£o
Aqui estÃ¡ como configurar seu ambiente de IA local do zero. Estes sÃ£o os passos exatos que eu useiâ€”apenas siga!
1. ğŸ”§ Instalar Docker Desktop
Abra o PowerShell ou CMD como Administrador.
Execute o seguinte comando para instalar o Docker:
winget install --id Docker.DockerDesktop --source winget

Reinicie sua mÃ¡quina apÃ³s a instalaÃ§Ã£o. Inicie o Docker Desktop, complete a configuraÃ§Ã£o inicial e certifique-se de que "Use the WSL 2 based engine" esteja habilitado.

2. ğŸ“¦ Criar um Volume Persistente para Modelos do Ollama
Isso garante que seus modelos baixados sobrevivam a paradas ou exclusÃµes de containers:
docker volume create ollama-data
Show Image
3. ğŸ¤– Executar o Ollama como um ServiÃ§o HTTP
Este comando executa o Ollama e o torna disponÃ­vel como uma API:
docker run -d --name ollama-server --gpus all -p 11434:11434 -v ollama-data:/root/.ollama -e OLLAMA_HOST="0.0.0.0:11434" ollama/ollama:latest serve
Show Image
ğŸ“˜ Detalhamento:

-d: Executar em segundo plano
--gpus all: Usar sua GPU se disponÃ­vel
-p 11434:11434: Expor a porta da API do Ollama
-v ollama-data:/root/.ollama: Usar seu volume persistente
-e OLLAMA_HOST: Fazer o Ollama escutar conexÃµes externas

ğŸ‘‰ Isso transforma o Ollama em um serviÃ§o de fornecimento de modelos com o qual ferramentas como Open-WebUI podem se comunicar.
4. ğŸ“¥ Baixar um Modelo via CLI do Ollama
Baixe seu modelo de IA preferido (exemplo abaixo usa phi4-mini-reasoning):
docker exec -it ollama-server ollama pull phi4-mini-reasoning:latest
ğŸ’¡ VocÃª pode substituir phi4-mini-reasoning:latest por qualquer outro modeloâ€”como llama3.1:8b. Confira a biblioteca de modelos do Ollama para mais opÃ§Ãµes em https://ollama.com/search
5. ğŸŒ Executar Open-WebUI e Conectar ao Ollama
Primeiro, crie um volume para armazenar os dados do WebUI:
docker volume create open-webui
Em seguida, execute o seguinte comando para iniciar a interface:
docker run -d --name open-webui --restart always --gpus all -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data -e PROVIDERS="ollama" -e OLLAMA_URL="http://host.docker.internal:11434" ghcr.io/open-webui/open-webui:cuda
ğŸ“˜ Detalhamento:

--restart always: ReinicializaÃ§Ã£o automÃ¡tica em caso de falha ou reinicializaÃ§Ã£o
--gpus all: Opcional, para melhor performance
-p 3000:8080: WebUI estarÃ¡ disponÃ­vel em http://localhost:3000
-v open-webui:/app/backend/data: ConfiguraÃ§Ãµes persistentes do WebUI
-e PROVIDERS="ollama": Definir Ollama como backend
-e OLLAMA_URL: Dizer ao WebUI onde encontrar seu servidor Ollama

6. ğŸ—¨ï¸ Navegue, Selecione e Gere!
Abra seu navegador e vÃ¡ para:
http://localhost:3000
Show Image
ğŸŒ Opcional: Compartilhamento Online com Ngrok
Quer permitir que amigos ou colegas usem sua IA remotamente? Use ngrok para criar um tÃºnel seguro para seu localhost.
Cadastre-se para uma conta gratuita aqui: https://ngrok.com
ğŸ“ Um guia completo sobre usar Ngrok estÃ¡ chegando em breve, mas sinta-se Ã  vontade para experimentar por conta prÃ³pria!
â“ Perguntas Frequentes
Q: Estou recebendo um erro "failed to write file: exit status 0xffffffff" no Ubuntu WSL. Qual Ã© a soluÃ§Ã£o?
A: Isso acontece quando o Docker nÃ£o consegue se conectar Ã  sua distro Ubuntu WSL. Aqui estÃ¡ como corrigir:
Desregistrar a distro com problema:
wsl --unregister Ubuntu
Reinstalar o Ubuntu:
wsl --install -d Ubuntu
VÃ¡ para Docker Desktop â†’ Settings â†’ Resources â†’ WSL Integration, e reative o Ubuntu.
Reinicie o Docker Desktop.
Q: Posso usar modelos diferentes?
Absolutamente! No passo 4, substitua phi4-mini-reasoning:latest por qualquer outro modelo como llama2:13b.
ğŸ‘‰ Confira a biblioteca de modelos do Ollama para mais opÃ§Ãµes!
Q: Como paro ou excluo os containers?
Para parar os containers:
docker stop ollama-server open-webui
Para removÃª-los:
docker rm ollama-server open-webui
ğŸ—‚ï¸ Nota: Seus dados estÃ£o seguros nos volumes, mesmo se os containers forem excluÃ­dos.
Q: E se eu nÃ£o tiver uma GPU?
Sem problemas! Apenas remova a flag --gpus all dos comandos Docker run.
âš ï¸ Vai executar na sua CPUâ€”mais lento, mas funciona!
ğŸ‘ CrÃ©ditos
Obrigado Ã s ferramentas incrÃ­veis que tornaram este projeto pessoal possÃ­vel:
ğŸ’¡ Ollama
ğŸ–¥ï¸ Open-WebUI
ğŸ³ Docker
ğŸŒ Ngrok
ğŸ’¬ Ã‰ isso! VocÃª agora tem um ambiente de IA local completo, privado, personalizÃ¡vel e extensÃ­vel. Aproveite seu prÃ³prio assistente similar ao ChatGPTâ€”nos seus prÃ³prios termos!
ğŸ’¡ Planejo no futuro mostrar como configurar e treinar os modelos de IA locais usando seus prÃ³prios dados! Acompanhe!
