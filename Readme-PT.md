# ğŸš€ Construa Seu PrÃ³prio Assistente de IA Local com Docker, Ollama & Open-WebUI

Configure seu prÃ³prio assistente privado similar ao ChatGPT localmente usando Ollama, Docker e Open-WebUI.
Este guia te leva atravÃ©s de uma configuraÃ§Ã£o comprovada e direta em apenas 6 passosâ€”testado tanto no Windows quanto no Linux.

---

## ğŸ“š Ãndice

- [ğŸ¯ O Que VocÃª Vai Conseguir](#-o-que-vocÃª-vai-conseguir)
- [ğŸ§° O Que Cada Ferramenta Faz](#-o-que-cada-ferramenta-faz)
- [ğŸ› ï¸ Passos da InstalaÃ§Ã£o](#ï¸-passos-da-instalaÃ§Ã£o)
- [ğŸŒ Opcional: Compartilhar Online com Ngrok](#-opcional-compartilhar-online-com-ngrok)
- [â“ Perguntas Frequentes](#-perguntas-frequentes)
- [ğŸ‘ CrÃ©ditos](#-crÃ©ditos)
- [ğŸ”® Em Breve](#-em-breve)

---

## ğŸ¯ O Que VocÃª Vai Conseguir

### ğŸ›¡ï¸ IA Focada na Privacidade
Mantenha seus dados locaisâ€”sem necessidade de nuvem. Suas conversas e dados ficam na sua mÃ¡quina, garantindo controle total e privacidade.

### ğŸ§  Flexibilidade de Modelos
Execute e alterne entre modelos como LLaMA 2, Mistral, ou qualquer modelo suportado pelo Ollama.

### ğŸ§© PersonalizaÃ§Ã£o
Use o Open-WebUI para dar memÃ³ria e personalidade Ã  sua IA. Personalize como sua IA se comporta e interage com vocÃª.

### ğŸ’¾ Armazenamento Persistente
Volumes do Docker mantÃªm seus modelos e dados segurosâ€”mesmo apÃ³s uma reinicializaÃ§Ã£o.

### ğŸŒ Compartilhamento Online Opcional
Use ferramentas como ngrok para compartilhar sua IA com seguranÃ§a com outros pela internet.

---

## ğŸ§° O Que Cada Ferramenta Faz

- **Docker**: Executa todos os componentes em containersâ€”portÃ¡vel e isolado.
- **Ollama**: Hospeda e serve seus modelos de IA localmente.
- **Open-WebUI**: Uma interface elegante no navegador para conversar com seus modelos.
- **Ngrok (opcional)**: Compartilha com seguranÃ§a seu assistente de IA local online.

---

## ğŸ› ï¸ Processo de InstalaÃ§Ã£o

Estes sÃ£o os passos exatos que estruturei atravÃ©s de mÃºltiplas configuraÃ§Ãµes de tentativa e erro (mais de cinco instalaÃ§Ãµes completas tanto no Windows quanto no Linux). Apenas siga em frente! PS: Este Ã© o processo de instalaÃ§Ã£o baseado no Windows

### ğŸ–¥ï¸ PrÃ©-requisitos & Testado Em

Aqui estÃ£o as especificaÃ§Ãµes do sistema e versÃµes usadas durante os testes. Outras configuraÃ§Ãµes podem funcionar, mas esta Ã© uma base comprovada:

| Componente | VersÃ£o MÃ­nima / Notas | Testado Em |
|------------|----------------------|------------|
| **Docker Desktop** | VersÃ£o estÃ¡vel mais recente | Windows 11 (WSL2) |
| **Docker CLI** | Vem com Docker Desktop | âœ… |
| **Ollama** | VersÃ£o estÃ¡vel mais recente | âœ… |
| **Open-WebUI** | VersÃ£o estÃ¡vel mais recente | âœ… |
| **GPU** | NVIDIA GTX 1650 4GB ou melhor | âœ… |
| **RAM** | 16 GB mÃ­nimo | 16 GB |
| **CPU** | Intel i3 (9Âª GeraÃ§Ã£o ou melhor) | Intel i3-9100F |

### âš™ï¸ ConfiguraÃ§Ã£o

Antes de executar os containers, aqui estÃ£o as principais variÃ¡veis de ambiente usadas:

| VariÃ¡vel | DescriÃ§Ã£o | Valor PadrÃ£o / Exemplo |
|----------|-----------|------------------------|
| `OLLAMA_HOST` | EndereÃ§o de escuta da API do Ollama | `0.0.0.0:11434` |
| `PROVIDERS` | Especifica backend para Open-WebUI | `ollama` |
| `OLLAMA_URL` | URL para conectar WebUI ao Ollama | `http://host.docker.internal:11434` |

---

## ğŸ› ï¸ Passos da InstalaÃ§Ã£o

### 1. ğŸ”§ Instalar Docker Desktop
Abra PowerShell ou CMD como Administrador.
Execute o seguinte comando para instalar o Docker:

```bash
winget install --id Docker.DockerDesktop --source winget
```

Reinicie sua mÃ¡quina apÃ³s a instalaÃ§Ã£o. Inicie o Docker Desktop, complete a configuraÃ§Ã£o da primeira execuÃ§Ã£o, e certifique-se de que "Use the WSL 2 based engine" esteja habilitado.

### 2. ğŸ“¦ Criar um Volume Persistente para Modelos do Ollama
Isso garante que seus modelos baixados sobrevivam a paradas ou exclusÃµes de containers:

```bash
docker volume create ollama-data
```

![image](https://github.com/user-attachments/assets/8da63b15-09b4-48e2-8716-9ec9660330b7)

### 3. ğŸ¤– Iniciar Ollama como um ServiÃ§o HTTP
Este comando executa o Ollama e o torna disponÃ­vel como uma API:

```bash
docker run -d --name ollama-server --restart always --gpus all -p 11434:11434 -v ollama-data:/root/.ollama -e OLLAMA_HOST="0.0.0.0:11434" ollama/ollama:latest serve
```

![image](https://github.com/user-attachments/assets/24ae70d9-86cd-4f16-aa0f-94c8993b39b2)

**ğŸ“˜ ExplicaÃ§Ã£o:**
- `-d`: Executar em segundo plano  
- `--gpus all`: Usar sua GPU se disponÃ­vel  
- `-p 11434:11434`: Expor porta da API do Ollama  
- `-v ollama-data:/root/.ollama`: Usar seu volume persistente  
- `-e OLLAMA_HOST`: Fazer Ollama escutar conexÃµes externas  

ğŸ‘‰ Isso transforma o Ollama em um serviÃ§o de servir modelos com o qual ferramentas como Open-WebUI podem se comunicar.

### 4. ğŸ“¥ Baixar um Modelo via CLI do Ollama
Baixe seu modelo de IA preferido (exemplo abaixo usa qwen3:1.7b):

```bash
docker exec -it ollama-server ollama pull qwen3:1.7b
```

ğŸ’¡ VocÃª pode trocar `qwen3:1.7b` por qualquer outro modeloâ€”como `llama3.1:8b`. Confira a biblioteca de modelos do Ollama para mais em https://ollama.com/search

### 5. ğŸŒ Iniciar Open-WebUI e Conectar ao Ollama
Primeiro, crie um volume para armazenar dados do WebUI:

```bash
docker volume create open-webui
```

EntÃ£o execute o seguinte comando para lanÃ§ar a interface:

```bash
docker run -d --name open-webui --restart always --gpus all -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data -e PROVIDERS="ollama" -e OLLAMA_URL="http://host.docker.internal:11434" ghcr.io/open-webui/open-webui:cuda
```

**ğŸ“˜ ExplicaÃ§Ã£o:**
- `--restart always`: Reinicia automaticamente em caso de crash ou reboot  
- `--gpus all`: Opcional, para melhor performance  
- `-p 3000:8080`: WebUI estarÃ¡ disponÃ­vel em http://localhost:3000  
- `-v open-webui:/app/backend/data`: ConfiguraÃ§Ãµes persistentes do WebUI  
- `-e PROVIDERS="ollama"`: Definir Ollama como backend  
- `-e OLLAMA_URL`: Dizer ao WebUI onde encontrar seu servidor Ollama

### 6. ğŸ—¨ï¸ Agora vocÃª pode conversar com seu prÃ³prio LLM Local!
Abra seu navegador e vÃ¡ para:

```
http://localhost:3000
```

![image](https://github.com/user-attachments/assets/ebc2a7e5-68c5-4e4f-9f1e-2d2e9fd0e0dd)

---

## ğŸŒ Opcional: Compartilhar Online com Ngrok

Quer deixar amigos ou colegas usarem sua IA remotamente? Use ngrok para criar um tÃºnel seguro para seu localhost.

Inscreva-se para uma conta gratuita aqui:
```
https://ngrok.com 
```

Crie um tÃºnel para a porta 3000:
```bash
ngrok http 3000
```

Isso te dÃ¡ um link temporÃ¡rio para acessar seu app local pela internet.

---

## â“ Perguntas Frequentes

**P: Estou recebendo um erro "failed to write file: exit status 0xffffffff" no Ubuntu WSL. Qual Ã© a soluÃ§Ã£o?**

R: Isso acontece quando o Docker nÃ£o consegue conectar Ã  sua distro Ubuntu WSL. Aqui estÃ¡ como consertar:

1. Desregistrar a distro quebrada:
   ```bash
   wsl --unregister Ubuntu
   ```
2. Re-instalar Ubuntu:
   ```bash
   wsl --install -d Ubuntu
   ```
3. VÃ¡ para Docker Desktop â†’ Settings â†’ Resources â†’ WSL Integration, e re-habilite Ubuntu.
4. Reinicie Docker Desktop.

**P: Posso usar modelos diferentes?**

Absolutamente! No passo 4, substitua `qwen3:1.7b` por qualquer outro modelo como `llama2:13b`.

ğŸ‘‰ Confira a biblioteca de modelos do Ollama para mais opÃ§Ãµes!

**P: Como parar ou deletar os containers?**

Para parar os containers:
```bash
docker stop ollama-server open-webui
```

Para removÃª-los:
```bash
docker rm ollama-server open-webui
```

ğŸ—‚ï¸ **Nota:** Seus dados estÃ£o seguros nos volumes, mesmo que os containers sejam deletados.

**P: E se eu nÃ£o tiver uma GPU?**

Sem problema! Apenas remova a flag `--gpus all` dos comandos Docker run.
âš ï¸ Vai executar na sua CPUâ€”mais lento, mas funciona!

---

## ğŸ‘ CrÃ©ditos

Obrigado Ã s ferramentas incrÃ­veis que tornaram este projeto pessoal possÃ­vel:

- ğŸ’¡ **Ollama**: https://ollama.com/ -- https://github.com/ollama/ollama
- ğŸ–¥ï¸ **Open-WebUI**: https://openwebui.com/ -- https://github.com/open-webui
- ğŸ³ **Docker**: https://www.docker.com/
- ğŸŒ **Ngrok**: https://ngrok.com/

---

## ğŸ”® Em Breve

- Como configurar memÃ³ria e conversas persistentes atravÃ©s das configuraÃ§Ãµes de arquivos RAG
- Como treinar modelos usando seus prÃ³prios documentos via fine-tuning usando unsloth (Em andamento)  
- Mais dicas de otimizaÃ§Ã£o e personalizaÃ§Ã£o!

---

ğŸ’¬ **Ã‰ isso!** VocÃª agora tem um ambiente de IA local completo, privado, personalizÃ¡vel e extensÃ­vel. Aproveite seu prÃ³prio assistente similar ao ChatGPTâ€”nos seus prÃ³prios termos!
