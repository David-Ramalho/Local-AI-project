# ğŸš€ Guide to Configuring RAG in Open WebUI with Ollama(OnProgress)
## ğŸ’¾ Low-VRAM Best Practices for 4GB GPUs
-  ğŸ¯ Perfect for:  Low-VRAM setups like GTX 1650 in my case. The better the GPU the Merrier! :)
 -  âš¡ Performance:  Optimized for single-container deployment. You can apply same logic in split containers as well 
 -  ğŸ“Š Reference Config:  Cogito 3B Q4 + znbang/bge:small-en-v1.5-q8_0

---

Retrieval-Augmented Generation (RAG) empowers your local AI to leverage external knowledge sources (documents, web data, etc.) for more accurate and contextually informed responses. This comprehensive guide provides battle-tested best practices for setting up RAG in Open WebUI using Ollama, specifically optimized for systems with limited GPU memory.

### ğŸ”§ **Quick Reference Configuration**
| Component | Setting | Value |
|-----------|---------|-------|
| ğŸ§  LLM Model | Cogito 3B Q4 | ~2.2GB VRAM |
| ğŸ“ Context Length | Extended | 8,048 tokens |
| ğŸ” Embedding Model | znbang/bge:small-en-v1.5-q8_0 | Batch size: 4 |
| ğŸ“„ Chunk Size | Optimal | 1,000 tokens |
| ğŸ”„ Chunk Overlap | Balanced | 100 tokens |
| ğŸ“Š Retrieval Top-K | Performance | 7 chunks |

---

## ğŸ“š Table of Contents

- [ğŸ¯ Step 1: Pulling and Configuring the LLM Model](#-step-1-pulling-and-configuring-the-llm-model)
- [ğŸ” Step 2: Setting the Embedding Model in Open WebUI](#-step-2-setting-the-embedding-model-in-open-webui)
- [ğŸ“ Step 3: Creating a Knowledge Base for Your Documents](#-step-3-creating-a-knowledge-base-for-your-documents)
- [âš™ï¸ Step 4: Indexing Documents (Chunking and Embedding)](#ï¸-step-4-indexing-documents-chunking-and-embedding)
- [ğŸ”— Step 5: Enabling RAG for the LLM (Connecting the Knowledge Base)](#-step-5-enabling-rag-for-the-llm-connecting-the-knowledge-base)
- [ğŸ§ª Step 6: Testing Retrieval and Tuning for Performance](#-step-6-testing-retrieval-and-tuning-for-performance)
- [ğŸ‰ Conclusion & Key Configuration Summary](#-conclusion--key-configuration-summary)
- [ğŸ“– Sources](#-sources)

---

## ğŸ¯ Step 1: Pulling and Configuring the LLM Model

The foundation of our RAG system starts with selecting an appropriate large language model that can efficiently handle retrieval tasks within our VRAM constraints.

### ğŸŒŸ **Recommended Model**
- Cogito 3B Q4_K_M - Our top choice for low-VRAM setups. This reasoning model delivers impressive performance while fitting comfortably within 4GB of VRAM.
Alternative Options.
- Qwen 3 1.7B - An even lighter option for extremely constrained hardware setups, though with reduced capabilities compared to the Cogito model.
- Qwen 3 4B - Offers enhanced intelligence and reasoning capabilities, but operates at the upper limit of 4GB VRAM capacity. 
### Settings for Cogito: 
- ğŸ“Š **Parameters:** 3.6B (4-bit quantized)
- ğŸ’¾ **Storage:** ~2.2GB on disk
- ğŸ”¥ **VRAM Usage:** Fits comfortably in 4GB
- ğŸ“ **Context:** Up to 128K tokens (we'll use ~8K for optimal performance)

### ğŸš€ **Installation**

```bash
# Pull the model
ollama pull cogito:3b

# Verify installation
ollama run cogito:3b -p "Hello, RAG world!"
```

### ğŸ›ï¸ **Why Extended Context Matters**

| Context Size | Use Case | RAG Suitability |
|--------------|----------|-----------------|
| 2,048 tokens | âŒ Default LLaMA | Too small for RAG |
| 4,096 tokens | âš ï¸ Basic tasks | Limited RAG capability |
| 8,192 tokens | âœ… **Recommended** | Perfect for multiple chunks |
| 16,384+ tokens | ğŸ”¥ Advanced | Overkill for most cases |

### ğŸ”€ **Alternative Models**

<details>
<summary><strong>ğŸ“‹ Click to expand alternative options</strong></summary>

| Model | Size | VRAM | Context | Notes |
|-------|------|------|---------|-------|
| **Mistral 7B Q4** | 7B | ~4GB | 4K-8K | Higher quality, tighter fit |
| **Llama2 7B Q4** | 7B | ~4GB | 4K | Classic choice |
| **Phi-3 Mini** | 3.8B | ~2GB | 128K | Microsoft's efficient model |
| **Gemma 2B** | 2B | ~1.5GB | 8K | Ultra-lightweight |

</details>

### âš¡ **Performance Expectations**

> **GTX 1650 Performance:**
> - ğŸ”„ **Generation Speed:** 2-4 tokens/second
> - â±ï¸ **First Token:** ~2-3 seconds
> - ğŸ“Š **VRAM Usage:** ~2.2GB for model + ~1GB for context and RAG

---

## ğŸ” Step 2: Setting the Embedding Model in Open WebUI

Configure your embedding engine for optimal document retrieval performance.

### ğŸ› ï¸ **Configuration Path**
```
Admin Settings â†’ Documents â†’ Embedding
```

### ğŸ¯ **Embedding Model Engine Setup**

#### **Option 1: Ollama API (Recommended)**
```
Endpoint: http://localhost:11434
âœ… Uses GPU acceleration
âœ… Consistent with LLM setup
âœ… Better performance
```

#### **Option 2: SentenceTransformers (Fallback)**
```
Engine: SentenceTransformers
âš ï¸ CPU-only processing
âš ï¸ Slower but more compatible
```

### ğŸŒŸ **Top Embedding Model Choices**

#### **ğŸ¥‡ Primary Recommendation**
**bge:small-en-v1.5-q8_0** (`bge:small-en-v1.5-q8_0`) For low VRam GPUS.
- ğŸ“Š **Size:** ~568M parameters
- ğŸ“ **Input Length:** 8,192 tokens
- ğŸ¯ **Specialty:** High-quality general embeddings
- ğŸ’¾ **VRAM:** ~1.2GB

```bash
ollama pull snowflake-arctic-embed2:latest
```

#### **ğŸ¥ˆ Alternative Choice**
**BAAI BGE M3** (`bge-m3:latest`)
- ğŸ“Š **Size:** Similar scale to Arctic
- ğŸŒ **Multilingual:** Excellent for non-English content
- ğŸ” **Features:** Dense + sparse + multi-vector retrieval

#### **ğŸ¥‰ Lightweight Option**
**MiniLM L12 v2** (`paraphrase-MiniLM-L12-v2`)
- ğŸ“Š **Size:** ~15M parameters (ultra-light)
- âš¡ **Speed:** Very fast processing
- ğŸ’¾ **VRAM:** Minimal usage

### ğŸ”§ **Critical Performance Settings**

| Setting | Recommended | Alternative | Impact |
|---------|-------------|-------------|---------|
| **Batch Size** | **4** | 2, 8 | 4Ã— speedup vs batch=1 |
| **Hybrid Search** | **âœ… Enabled** | Disabled | +20% retrieval quality |
| **Re-ranker** | **âŒ Disabled** | Enabled | Saves ~500MB VRAM |

### ğŸ’¡ **Memory Optimization Tips**

<details>
<summary><strong>ğŸ” VRAM Breakdown Analysis</strong></summary>

```
Total VRAM Usage Estimate:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Component               â”‚ Usage    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cogito 3B Model         â”‚ ~2.2GB   â”‚
â”‚ Arctic Embed v2         â”‚ ~1.2GB   â”‚
â”‚ Context Buffer          â”‚ ~0.4GB   â”‚
â”‚ System Overhead         â”‚ ~0.2GB   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ **Total**               â”‚ **4.0GB**â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</details>

---

## ğŸ“ Step 3: Creating a Knowledge Base for Your Documents

Set up your document repository for RAG integration.

### ğŸ¯ **Step-by-Step Creation**

1. **ğŸ§­ Navigate:** `Workspace â†’ Knowledge`
2. **â• Create:** Click `"+ Create a Knowledge Base"`
3. **ğŸ·ï¸ Name:** Choose descriptive name (e.g., `TechDocs-RAG`, `CompanyKB`)
4. **ğŸ¯ Purpose:** Select appropriate category:
   - ğŸ“‹ **Assistance** - General help and support
   - â“ **Q&A** - FAQ and question answering
   - ğŸ“š **Reference** - Technical documentation
   - ğŸ“ **Learning** - Educational materials

### ğŸ’¡ **Naming Best Practices**

| âœ… Good Examples | âŒ Avoid |
|------------------|---------|
| `API-Documentation-v2024` | `docs` |
| `Customer-Support-KB` | `kb1` |
| `Technical-Manuals-Engineering` | `stuff` |

---

## âš™ï¸ Step 4: Indexing Documents (Chunking and Embedding)

Transform your documents into searchable, AI-ready knowledge chunks.

### ğŸ“¤ **Document Upload Process**

1. **ğŸ“ Access:** `Knowledge â†’ [Your KB Name]`
2. **â• Upload:** Click `"+"` button
3. **ğŸ“„ Formats Supported:**
   - âœ… **Markdown** (`.md`)
   - âœ… **Text Files** (`.txt`)
   - âœ… **PDFs** (basic + Tika-enhanced)
   - âœ… **Word Documents** (`.docx`)
   - âœ… **HTML Files**

### ğŸ”§ **Optimal Chunking Configuration**

| Parameter | Value | Reasoning |
|-----------|-------|-----------|
| **Tokenizer** | `tiktoken` | OpenAI-compatible, accurate |
| **Chunk Size** | **1,000 tokens** | ~750-800 words, optimal balance |
| **Overlap** | **100 tokens** | 10% overlap prevents context loss |

### ğŸ“Š **Chunking Strategy Deep Dive**

<details>
<summary><strong>ğŸ§  Understanding the Trade-offs</strong></summary>

#### **Smaller Chunks (500-750 tokens)**
- âœ… More precise retrieval
- âœ… Better for specific facts
- âŒ May lose broader context
- âŒ More chunks = more embeddings

#### **Larger Chunks (1200-1500 tokens)**
- âœ… Preserves more context
- âœ… Fewer embeddings needed
- âŒ Less precise matching
- âŒ May exceed embedding model limits

#### **Our Sweet Spot (1000 tokens)**
- ğŸ¯ Balances precision and context
- ğŸ¯ Fits well within embedding limits
- ğŸ¯ Optimal for most document types

</details>

### ğŸ”„ **Indexing Process Monitoring**

```
Progress Indicators:
ğŸ“ Document Processing    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 80%
ğŸ” Chunk Generation      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 70%
ğŸ§® Embedding Creation    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 60%
ğŸ’¾ Vector Storage        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 50%
```

### ğŸš¨ **Common Issues & Solutions**

| Issue | Solution |
|-------|----------|
| **PDF Text Garbled** | Enable Apache Tika in Admin Settings |
| **Chunk Size Error** | Reduce chunk size to fit embedding model |
| **Slow Processing** | Lower embedding batch size |
| **Memory Issues** | Process fewer documents simultaneously |

---

## ğŸ”— Step 5: Enabling RAG for the LLM (Connecting the Knowledge Base)

Bridge your language model with your knowledge repository.

### ğŸ¯ **Model Configuration Steps**

1. **ğŸ§­ Navigate:** `Workspace â†’ Models`
2. **â• Create:** Click `"+ Add New Model"`
3. **ğŸ·ï¸ Name:** Descriptive identifier (e.g., `Cogito-3B-TechDocs-RAG`)
4. **ğŸ§  Base Model:** Select `cogito:3b`
5. **ğŸ“š Knowledge Source:** Link your created knowledge base
6. **ğŸ’¾ Save:** Confirm configuration

### âš™ï¸ **Critical Retrieval Settings**

| Setting | Recommended | Range | Impact |
|---------|-------------|-------|---------|
| **Top K** | **7** | 3-10 | Number of chunks retrieved |
| **Score Threshold** | **0.1** | 0.0-0.5 | Minimum relevance score |
| **Hybrid Search** | **âœ… On** | On/Off | Keyword + semantic search |
| **Re-ranking** | **âŒ Off** | On/Off | Advanced relevance sorting |

### ğŸ“ **Context Math**

```
Context Allocation (8,048 tokens):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Component           â”‚ Tokens     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Retrieved Chunks    â”‚ ~7,000     â”‚
â”‚ (7 Ã— 1,000 tokens)  â”‚            â”‚
â”‚ User Question       â”‚ ~50-200    â”‚
â”‚ System Instructions â”‚ ~300-500   â”‚
â”‚ Response Buffer     â”‚ ~348-698   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ **Total**          â”‚ **8,048**  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¨ **Custom Prompt Template**

The default RAG template structure:
```
ğŸ“‹ Reference Materials:
[Retrieved Chunk 1]
[Retrieved Chunk 2]
...
[Retrieved Chunk N]

â“ Question: {user_question}
```

**Customization Path:** `Admin Settings â†’ Documents â†’ RAG Template`

---

## ğŸ§ª Step 6: Testing Retrieval and Tuning for Performance

Validate and optimize your RAG system for production use.

### âœ… **Basic Functionality Test**

1. **ğŸ’¬ Start Chat:** Create new chat with your RAG-enabled model
2. **â“ Test Query:** Ask about content from your documents
   ```
   Example: "What are the installation requirements mentioned in the setup guide?"
   ```
3. **ğŸ” Verify Citations:** Look for reference markers like **ã€Doc1â€ L10-L20ã€‘**

### ğŸ“Š **Performance Benchmarks**

| Metric | Target | Acceptable | Poor |
|--------|--------|------------|------|
| **Retrieval Time** | <1s | <2s | >3s |
| **First Token** | <3s | <5s | >8s |
| **Generation Speed** | 2-4 tok/s | 1-2 tok/s | <1 tok/s |
| **Citation Accuracy** | >90% | >70% | <50% |

### ğŸ›ï¸ **Quality Tuning Matrix**

| Problem | Likely Cause | Solution |
|---------|--------------|----------|
| **Missing Information** | Low Top-K or poor chunks | â†—ï¸ Increase Top-K to 8-10 |
| **Irrelevant Results** | No score threshold | â†—ï¸ Set threshold to 0.1-0.2 |
| **Hallucinations** | Weak instruction | â†—ï¸ Add "Only use provided docs" |
| **Poor Citations** | Citation settings off | â†—ï¸ Enable RAG citations |
| **Slow Performance** | High batch/context | â†˜ï¸ Reduce batch size or Top-K |

### ğŸ”§ **Advanced Optimization**

<details>
<summary><strong>ğŸš€ Performance Tuning Checklist</strong></summary>

#### **Memory Optimization**
- [ ] Monitor VRAM usage with `nvidia-smi`
- [ ] Adjust batch size based on available memory
- [ ] Consider CPU fallback for embedding if needed

#### **Quality Improvement**
- [ ] Test with out-of-document queries
- [ ] Verify multi-document retrieval works
- [ ] Experiment with different prompt templates

#### **Speed Enhancement**
- [ ] Enable streaming responses
- [ ] Optimize chunk size for your use case
- [ ] Consider model quantization levels

</details>

### ğŸš¨ **Troubleshooting Guide**

<details>
<summary><strong>ğŸ”§ Common Issues and Quick Fixes</strong></summary>

| ğŸš¨ Issue | ğŸ” Diagnosis | âœ… Solution |
|----------|--------------|-------------|
| **No retrieval happening** | Context too small or KB not linked | Verify 8K+ context, check KB connection |
| **Context length exceeded** | Too many/large chunks | Reduce Top-K or chunk size |
| **OOM during generation** | VRAM exhausted | Lower settings or use CPU fallback |
| **Embedding OOM** | Batch size too high | Reduce batch to 2 or 1 |
| **Terse responses** | Default instructions too limiting | Add "detailed answer" to prompt |
| **No citations** | Citation feature disabled | Enable RAG citations in settings |

</details>

---

## ğŸ‰ Conclusion & Key Configuration Summary

### ğŸ¯ **Optimal Configuration Recap**

| Component | Setting | Value | Purpose |
|-----------|---------|-------|---------|
| ğŸ§  **LLM Model** | Cogito 3B Q4 | 8K context | Efficient reasoning |
| ğŸ” **Embedding** | Arctic Embed v2 | Batch: 4 | Quality retrieval |
| ğŸ“„ **Chunking** | Smart split | 1K tokens, 100 overlap | Balanced context |
| ğŸ“Š **Retrieval** | Hybrid search | Top-K: 7 | Comprehensive results |
| ğŸ’¾ **Memory** | Optimized | ~4GB total | GTX 1650 compatible |

### ğŸš€ **System Capabilities**

Your fully configured RAG system now provides:

- âœ… **Document-Grounded Responses** with proper citations
- âœ… **Multi-Source Information Synthesis** across your knowledge base
- âœ… **Memory-Efficient Operation** on consumer hardware
- âœ… **Real-Time Retrieval** with sub-second response times
- âœ… **Scalable Architecture** for growing document collections

### ğŸŠ **You're Ready!**

Congratulations! You now have a production-ready, locally-hosted RAG system that rivals cloud solutions while maintaining complete data privacy and control. Your AI assistant is now equipped with your domain knowledge and ready to provide accurate, contextual responses.

---

## ğŸ“– Sources

### ğŸ“š **Official Documentation**
- [Open WebUI RAG Features](https://docs.openwebui.com/features/rag/) - Core RAG functionality
- [Ollama Model Library](https://ollama.com/library/) - Model repository and specs

### ğŸ§  **Model Resources**
- [Cogito 3B Model Page](https://ollama.com/library/cogito:3b) - Primary LLM documentation
- [znbang/bge:small-en-v1.5-q8_0]((https://ollama.com/znbang/bge:small-en-v1.5-q8_0)) - Embedding model details
- [BGE M3 Documentation](https://huggingface.co/BAAI/bge-m3) - Alternative embedding model

### ğŸ’¡ **Community Resources**
- **Reddit Discussions:** r/LocalLLaMA RAG configuration threads
- **GitHub Issues:** Open WebUI repository troubleshooting
- **Medium Article:** "Multi-Source RAG with Hybrid Search and Re-ranking in OpenWebUI" by Richard Meyer (2025)

### ğŸ› ï¸ **Technical References**
- **Memory Optimization:** Community benchmarks and VRAM usage studies
- **Chunking Strategies:** Academic papers on optimal text segmentation
- **Performance Tuning:** User-contributed optimization guides

---

<div align="center">

### ğŸŒŸ **Happy RAG-ing!** ğŸŒŸ

*Built with â¤ï¸ for the local AI community*

[![GitHub](https://img.shields.io/badge/GitHub-Contribute-green?style=for-the-badge&logo=github)](https://github.com)
[![OpenWebUI](https://img.shields.io/badge/OpenWebUI-Docs-blue?style=for-the-badge)](https://docs.openwebui.com)
[![Ollama](https://img.shields.io/badge/Ollama-Models-orange?style=for-the-badge)](https://ollama.com)

</div>


Research paper:

````markdown
# Guide to Configuring RAG in Open WebUI with Ollama (Lowâ€‘VRAM Best Practices)

Retrievalâ€‘Augmented Generation (RAG) allows your local AI to use external knowledge (documents, web data, etc.) to produce more accurate and informed answers. This guide walks through best practices for setting up RAG in Open WebUI using Ollama on a singleâ€‘container setup â€“ ideal for a single 4â€¯GB VRAM GPU (e.g. GTXâ€¯1650). We use a reference configuration (embedding batch size 4, chunk size 1000, chunk overlap 100, retrieval top_kâ€¯7, contextâ€¯â‰ˆ8048 tokens) and the Cogitoâ€¯3Bâ€¯Q4 model as an example. Each step includes recommended settings, model choices, performance impacts, and troubleshooting tips.

---

## Table of Contents

1. [Pulling and Configuring the LLM Model](#step-1-pulling-and-configuring-the-llm-model)  
2. [Setting the Embedding Model in Open WebUI](#step-2-setting-the-embedding-model-in-open-webui)  
3. [Creating a Knowledge Base for Your Documents](#step-3-creating-a-knowledge-base-for-your-documents)  
4. [Indexing Documents (Chunking and Embedding)](#step-4-indexing-documents-chunking-and-embedding)  
5. [Enabling RAG for the LLM (Connecting the Knowledge Base)](#step-5-enabling-rag-for-the-llm-connecting-the-knowledge-base)  
6. [Testing Retrieval and Tuning for Performance](#step-6-testing-retrieval-and-tuning-for-performance)  
7. [Conclusion & Key Configuration Summary](#conclusion--key-configuration-summary)  
8. [Sources](#sources)  

---

## Stepâ€¯1: Pulling and Configuring the LLM Model

The first step is to obtain a suitable large language model (LLM) that can handle RAG on limited GPU memory.

### Recommended Model

- **Cogitoâ€¯3B Q4\_K\_M** â€“ a 3.6â€¯Bâ€‘parameter model quantized to 4â€‘bit (â‰ˆ2.2â€¯GB on disk)  
  - Supports up to 128â€¯K tokens of context (weâ€™ll use ~8â€¯K tokens / 8048)  
  - Fits comfortably in 4â€¯GB of VRAM

### Why Extended Context?

- Default LLaMA models have a 2048â€‘token window, too short for RAG  
- Increasing to ~8192 tokens ensures space for multiple 1000â€‘token chunks + query overhead

### How to Pull

```bash
ollama pull cogito:3b
````

> Tip: After pulling, verify with:
>
> ```bash
> ollama run cogito:3b -p "Hello"
> ```

### Alternate Models

* **Mistralâ€¯7B 4â€‘bit** or **Llama2â€¯7Bâ€¯Q4** (may need to lower context to 4096)
* Smaller (2â€“4â€¯B) run faster but can be less capable
* Balance VRAM use vs. accuracy

### Model Settings

* Set the context length to **8192** (or **8048** for comfort)
* In Open WebUI, ensure the modelâ€™s max context reflects this setting

### Performance Impact & Troubleshooting

* Expect a few tokens/sec on GTXâ€¯1650
* Extended context increases KVâ€‘cache size and slightly reduces speed
* If outâ€‘ofâ€‘memory, choose a smaller/ moreâ€‘aggressive quantized model (e.g. 3â€‘bit) or run CPUâ€‘only (slow)

---

## Stepâ€¯2: Setting the Embedding Model in Open WebUI

Configure your embedding engine in **Admin Settingsâ€¯>â€¯Documents**.

### Embedding Model Engine

* **Ollama API endpoint** (e.g. `http://localhost:11434`) â€“ uses GPU for speed
* Fallback: SentenceTransformers on CPU (slower)

### Embedding Model Choice

1. **bge:small-en-v1.5-q8_0** (`bge:small-en-v1.5-q8_0`)

   * \~568â€¯M params, 8192â€‘token input length
2. **BAAI BGE M3** (`BAAI/bge-m3`)

   * Similar scale and token length

> Lighter option: `paraphraseâ€‘MiniLMâ€‘L12â€‘v2` (\~15â€¯M params)

### Batch Size

* **4** (safe on 4â€¯GB, 4Ã— speedup)
* Try **8** if headroom; drop to **2** or **1** if OOM

### Hybrid Search

* **Enable** for keyword + dense retrieval
* BM25 runs on CPU, minimal GPU impact

### Reâ€‘ranker Model (Optional)

* Crossâ€‘encoder (e.g. `bge-reranker-v2-m3`) can improve accuracy
* **Off** by default on 4â€¯GB to conserve resources

### Memory & Performance

* Embed model (\~1.2â€¯GB) + Cogito (\~2.2â€¯GB) â‰ˆâ€¯3.4â€¯GB total
* If OOM, run embed on CPU or choose a smaller model

---

## Stepâ€¯3: Creating a Knowledge Base for Your Documents

1. **Workspaceâ€¯>â€¯Knowledge**
2. Click **â€œ+ Create a Knowledge Baseâ€**
3. Name it (e.g. `MyDocs` or `Tech Manuals KB`)
4. Choose a Purpose (e.g. `Assistance`, `QA`, `Reference`)
5. Save and confirm

> Ensure **chunking** and **embedding** settings (from Stepsâ€¯1â€“2) are in place before indexing.

---

## Stepâ€¯4: Indexing Documents (Chunking and Embedding)

### Uploading Documents

* **Knowledgeâ€¯>â€¯\[Your KB]**
* Use **â€œ+â€** to upload files or folders (Markdown, text, PDF, etc.)
* For complex PDFs, enable Apache Tika in Admin Settings

### Chunking Strategy

* **Tokenizer**: Tiktoken
* **Chunk Size**: **1000** tokens (\~750â€“800 words)
* **Overlap**: **100** tokens (10% overlap)

> Tradeâ€‘off:
>
> * Smaller chunks â†’ more focused retrieval
> * Larger chunks â†’ fewer but richer context blocks

### Indexing Process

1. Files are split into chunks
2. Each chunk is embedded (batch size 4)
3. Vectors stored in the index

Monitor progress in UI or via Docker logs.

### Troubleshooting

* **Unsupported formats** â†’ convert or enable Tika
* **Chunk too large** â†’ reduce size to fit embed modelâ€™s limit
* **Slow indexing** â†’ check that GPU embed is enabled; lower batch if VRAM is maxed

---

## Stepâ€¯5: Enabling RAG for the LLM (Connecting the Knowledge Base)

1. **Workspaceâ€¯>â€¯Models**
2. Click **â€œ+ Add New Modelâ€**
3. Name it (e.g. `CogitoÂ 3BÂ +Â MyDocs (RAG)`)
4. **Base Model**: select your pulled Cogitoâ€¯3B
5. **Knowledge Source**: select `MyDocs`
6. Save/confirm

### Retrieval Settings

* **Topâ€¯K**: **7** (â‰ˆ7000â€¯tokens â†’ fits in 8048)
* **Min Score Threshold**: **0â€“0.1** (optional)
* **Hybrid Search**: on (BM25 + embedding)
* **Reâ€‘ranking**: off by default on 4â€¯GB

### Prompt Template

* Default RAG template prepends:

  ```
  Reference:
  <chunk 1>
  <chunk 2>
  â€¦
  Question: {user question}
  ```
* Customize in **Admin Settingsâ€¯>â€¯Documentsâ€¯>â€¯RAG Template**

---

## Stepâ€¯6: Testing Retrieval and Tuning for Performance

### 6.1 Basic Functionality

1. **Chatâ€¯>â€¯New Chat** with `CogitoÂ 3BÂ +Â MyDocs (RAG)`
2. Ask a question in your docs (e.g. â€œWhat does the installation section sayâ€¦?â€)
3. Look for citation markers (e.g. **ã€Doc1â€ L10-L20ã€‘**)

### 6.2 Performance & Latency

* **Retrieval**: <â€¯1â€¯s
* **Generation**: \~5â€“10â€¯s for \~100 tokens on GTXâ€¯1650
* If slow: check GPU usage, reâ€‘ranks, streaming settings

### 6.3 Quality Tuning

* **Omit info** â†’ raise Topâ€¯K or enable reâ€‘ranker
* **Extraneous info** â†’ tighten score threshold or instructions
* **Hallucinations** â†’ add â€œAnswer only from documentsâ€ in prompt
* **Citation issues** â†’ ensure RAG citations are enabled

### 6.4 Balancing Quality & Speed

* Prefer small RAG model (3â€¯B + docs) over large nonâ€‘RAG model
* Monitor `nvidia-smi` for VRAM peaks
* Adjust chunk size, Topâ€¯K, batch, and embedding model as needed

### 6.5 Common Issues & Fixes

| Issue                   | Fix                                                         |
| ----------------------- | ----------------------------------------------------------- |
| No retrieval            | Confirm context â‰¥â€¯8192, KB linked, reâ€‘index if necessary    |
| Contextâ€‘length exceeded | Reduce Topâ€¯K or chunk size                                  |
| OOM during generation   | Lower chunk/Topâ€¯K, unload other models, or use CPU fallback |
| Embedding OOM           | Lower batch size or run embed on CPU                        |
| Terse answers           | Add â€œGive a detailed answer with referencesâ€ to prompt      |

### 6.6 Ongoing Verification

* **Outâ€‘ofâ€‘docs query**: see if it answers or states â€œnot in documentsâ€
* **Multiâ€‘part queries**: ensure it retrieves from all relevant docs
* **System prompts**: experiment with stronger instructions for citation

---

## Conclusion & Key Configuration Summary

* **LLM Model**: Cogitoâ€¯3Bâ€¯Q4, \~8â€¯K context
* **Embedding Model**: Snowflake Arctic Embedâ€¯v2 or BGEâ€¯M3 via Ollama
* **Chunking**: 1000 tokens, 100 overlap
* **Retrieval**: Topâ€¯Kâ€¯7, hybrid search on, reâ€‘rank off
* **Batching**: embed batch size 4
* **Context Size**: \~8048 tokens

By following these steps, youâ€™ll have a fully local RAG system on Open WebUI + Ollama running comfortably on a 4â€¯GB GTXâ€¯1650. Enjoy your grounded, documentâ€‘aware AI assistant!

---

## Sources

* [Open WebUI RAG Overview](https://docs.openwebui.com/features/rag/)
* [Ollama Cogitoâ€¯3B Model](https://ollama.com/library/cogito:3b)
* [Ollama Arctic Embed v2](https://ollama.com/library/snowflake-arctic-embed2)
* Reddit user discussions on Open WebUI RAG settings
* Medium: â€œMultiâ€‘Source RAG with Hybrid Search and Reâ€‘ranking in OpenWebUIâ€ (Richard Meyer, 2025)
* Various GitHub issues and discussions on batch size, memory tuning, and troubleshooting

```
```
