# ğŸš€ Split Chat & Embeddings for Maximum Performance

**Transform your existing Ollama + Open-WebUI setup into a high-performance, dual-container powerhouse!** 

This guide shows you how to migrate from a single-container setup to a specialized architecture that prevents GPU memory bottlenecks and delivers lightning-fast performance for both chat and document processing.

---

## ğŸ¯ What You'll Achieve

### ğŸš« **No More GPU Memory Swapping**
Stop waiting 10-30 seconds every time you switch between chat models and document embeddings. Your GPU stays dedicated to what it does bestâ€”running chat models fast.

### âš¡ **Instant Model Switching** 
Switch between LLaMA, Qwen, Mistral, or any model instantly. No more "loading..." delays when your GPU runs out of memory.

### ğŸ§  **Parallel Processing Power**
Chat with your AI *while* it processes documents in the background. True multitasking that keeps both workflows smooth.

### ğŸ¯ **Optimized Resource Usage**
- **GPU Container**: 100% focused on chat models (Qwen, LLaMA, etc.)
- **CPU Container**: Dedicated to embeddings and document processing
- **Your Experience**: Seamless, fast, no compromises

### ğŸ’¾ **Zero Data Loss Migration**
Keep everything you've builtâ€”your models, conversations, and document indexes stay intact. This is an upgrade, not a rebuild.

---

## ğŸ§° Why This Architecture Rocks

**Before (Single Container):**
```
ğŸ”„ Chat Model â†”ï¸ Embedding Model (GPU Memory Swap Hell)
â±ï¸  Switch time: 10-30 seconds
ğŸ˜¤ Experience: Frustrating waits
```

**After (Dual Container):**
```
ğŸ’¬ GPU Container: Chat Models (Always Ready)
ğŸ“„ CPU Container: Embeddings (Always Available)  
â±ï¸  Switch time: Instant
ğŸ˜ Experience: Smooth as butter
```

---

## ğŸ–¥ï¸ Prerequisites & Tested On

| Component        | Minimum Required    | Tested & Proven On         |
|------------------|--------------------|-----------------------------|
| **Docker CLI**   | v24.x or newer     | Ubuntu 22.04, Windows 11   |
| **Ollama**       | Latest stable      | v0.4.1 âœ…                   |
| **Open-WebUI**   | Latest stable      | v1.2.0 âœ…                   |
| **GPU** (optional) | GTX 1650 (4GB+) | GTX 1650 âœ…                 |
| **CPU**          | 4+ cores           | Intel i3-9100F âœ…           |
| **RAM**          | 8GB minimum        | 16GB recommended âœ…          |

---

## âš™ï¸ Configuration Variables

| Variable          | What It Does                           | Value You'll Use                 |
|-------------------|----------------------------------------|----------------------------------|
| `OLLAMA_HOST`     | Makes Ollama listen for connections    | `0.0.0.0:11434`                |
| `PROVIDERS`       | Tells WebUI which backend to use       | `ollama`                        |
| `OLLAMA_URL`      | Main endpoint (GPU chat models)       | `http://host.docker.internal:11434` |
| `SECONDARY_URL`   | Secondary endpoint (CPU embeddings)   | `http://host.docker.internal:11435` |

---

## ğŸ” Before We Start: Check Your Current Setup

Run these commands to see what you're working with:

```bash
# 1. See your running containers
docker ps

# 2. List your downloaded models  
docker exec -it ollama-server ollama list

# 3. Check GPU status (if you have one)
nvidia-smi
```

> **ğŸ’¬ Tell me what you see!** This helps ensure we're starting from the right place.

---

## ğŸ› ï¸ Migration Steps

### Step 1: ğŸ–¥ï¸ Create Your CPU-Only Embeddings Container

Launch a dedicated container just for embeddingsâ€”no GPU needed, pure CPU power:

```bash
docker run -d \
  --name ollama-embeddings \
  -p 11435:11434 \
  -v ollama-data:/root/.ollama \
  -e OLLAMA_HOST="0.0.0.0:11434" \
  ollama/ollama:latest serve
```

**ğŸ” What's happening:**
- **No `--gpus` flag** = CPU-only operation  
- **Port 11435** = Your new embeddings endpoint
- **Same volume** = Access to all your existing models
- **Zero downtime** = Your original setup keeps running

**âœ… Verify it's running:**
```bash
docker ps  # You should see both ollama-server AND ollama-embeddings
```

---

### Step 2: ğŸ“¥ Download Embedding Model to CPU Container

Pull your embedding model directly into the CPU container:

```bash
docker exec -it ollama-embeddings \
  ollama pull znbang/bge:large-en-v1.5-f16
```

**ğŸ¯ Why this model?** It's optimized for document processing and runs great on CPU.

**âœ… Verify both containers have models:**
```bash
# Check CPU container
docker exec -it ollama-embeddings ollama list

# Check GPU container  
docker exec -it ollama-server ollama list
```

---

### Step 3: âš¡ Optimize Your GPU Container

Time to upgrade your GPU container for peak performance:

```bash
# Gracefully stop and remove old container
docker stop ollama-server && docker rm ollama-server

# Create optimized GPU container
docker run -d \
  --name ollama-gpu \
  --gpus all \
  -p 11434:11434 \
  -v ollama-data:/root/.ollama \
  -e OLLAMA_HOST="0.0.0.0:11434" \
  -e CUDA_VISIBLE_DEVICES=0 \
  ollama/ollama:latest serve
```

**ğŸš€ Performance boost:** Same port (11434), but now optimized exclusively for chat models!

---

### Step 4: ğŸŒ Upgrade Open-WebUI for Dual-Container Power

Stop your current WebUI and create a new one that knows about both containers:

```bash
# Stop current WebUI
docker stop open-webui && docker rm open-webui

# Launch upgraded WebUI
docker run -d \
  --name open-webui \
  --restart always \
  --gpus all \
  -p 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  -e PROVIDERS="ollama" \
  -e OLLAMA_URL="http://host.docker.internal:11434" \
  ghcr.io/open-webui/open-webui:cuda
```

**ğŸ’¡ Same interface, supercharged backend!**

---

### Step 5: ğŸ”— Configure Dual Endpoints

1. Open your browser: **http://localhost:3000**
2. Go to âš™ï¸ **Settings** â†’ **Admin Panel** â†’ **Connections**
3. Set **Ollama API Base URL** to:
   ```
   http://host.docker.internal:11434,http://host.docker.internal:11435
   ```

**ğŸ¯ What this does:**
- **11434** (GPU) = Chat models get priority
- **11435** (CPU) = Embeddings run separately
- **Result** = No more memory conflicts!

---

### Step 6: ğŸ§¹ Clean Up GPU Memory

Free up GPU space by removing the embedding model from your GPU container:

```bash
docker exec -it ollama-gpu ollama rm znbang/bge:large-en-v1.5-f16
```

**ğŸ’¾ Don't worry!** Your embeddings are safe on the CPU container, and your document indexes remain intact.

---

## ğŸ—ï¸ Your New Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Open-WebUI    â”‚   â”‚   Ollama-GPU    â”‚   â”‚  Ollama-CPU     â”‚
â”‚   Port 3000     â”‚â”€â”€â–¶â”‚   Port 11434    â”‚   â”‚  Port 11435     â”‚
â”‚   Interface     â”‚   â”‚   Chat Models   â”‚   â”‚  Embeddings     â”‚
â”‚   --gpus all    â”‚   â”‚   --gpus all    â”‚   â”‚  (CPU only)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â”‚                       â–¼                       â–¼
        â”‚              ğŸš€ Lightning Chat         ğŸ“„ Smart Docs
        â”‚              (GPU Accelerated)        (CPU Optimized)
        â”‚                       â”‚                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           Seamless Experience
```

---

## ğŸ§ª Test Your New Setup

### ğŸ’¬ **Chat Test**
1. Select any chat model (Qwen, LLaMA, etc.)
2. Notice the **instant response** times
3. Try switching between modelsâ€”**zero delay!**

### ğŸ“„ **Document Processing Test** 
1. Upload a PDF or document
2. Ask questions about it
3. Watch as embeddings process on CPU **while you keep chatting**

### ğŸ”„ **Multitasking Test**
Chat with your AI while uploading/processing documentsâ€”both work simultaneously!

---

## âœ… Verification Commands

Check that everything's running perfectly:

```bash
# See all containers
docker ps

# Check GPU usage
nvidia-smi

# List models in GPU container
docker exec -it ollama-gpu ollama list

# List models in CPU container  
docker exec -it ollama-embeddings ollama list

# Test endpoints
curl http://localhost:11434/api/tags  # GPU endpoint
curl http://localhost:11435/api/tags  # CPU endpoint

# Check WebUI logs
docker logs open-webui
```

---

## ğŸ› ï¸ Troubleshooting

### ğŸ” **Model Not Found**
```bash
# Check which container has which models
docker exec -it ollama-gpu ollama list
docker exec -it ollama-embeddings ollama list
```

### ğŸŒ **Connection Issues**
```bash
# Test endpoints individually
curl http://localhost:11434/api/tags
curl http://localhost:11435/api/tags
```

### ğŸ”„ **Need a Fresh Start?**
```bash
docker restart ollama-gpu ollama-embeddings open-webui
```

---

## â†©ï¸ Rollback Plan (If Needed)

Changed your mind? Here's how to go back to your original setup:

```bash
# Stop new containers
docker stop ollama-gpu ollama-embeddings open-webui
docker rm ollama-gpu ollama-embeddings open-webui

# Restore original setup
docker run -d --name ollama-server --gpus all -p 11434:11434 \
  -v ollama-data:/root/.ollama -e OLLAMA_HOST="0.0.0.0:11434" \
  ollama/ollama:latest serve

docker run -d --name open-webui --restart always --gpus all -p 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data -e PROVIDERS="ollama" \
  -e OLLAMA_URL="http://host.docker.internal:11434" \
  ghcr.io/open-webui/open-webui:cuda
```

**ğŸ’¾ Your data stays safe!** All models, conversations, and settings are preserved.

---

## ğŸ‰ What You'll Notice Immediately

### âœ… **Before vs After**

| Experience | Before (Single) | After (Dual) |
|------------|----------------|--------------|
| **Model Switching** | 10-30 seconds â³ | Instant âš¡ |
| **Chat + Docs** | One at a time ğŸ˜” | Parallel processing ğŸš€ |
| **GPU Memory** | Constant swapping ğŸ”„ | Stable usage ğŸ“Š |
| **Overall Feel** | Sluggish waits ğŸ˜¤ | Smooth & responsive ğŸ˜ |

### ğŸš€ **Performance Wins**
- **Instant model switching**â€”no more coffee breaks while waiting
- **Parallel processing**â€”chat while docs process in background  
- **Stable GPU usage**â€”dedicated containers, optimized performance
- **Same beloved interface**â€”just supercharged under the hood

---

## ğŸŠ Congratulations!

You've just transformed your local AI setup into a high-performance, dual-container powerhouse! 

**Enjoy your lightning-fast, multi-tasking AI assistant that never keeps you waiting!** âš¡ğŸ¤–

---

*ğŸ’¬ Questions? Issues? Drop them belowâ€”this setup has been battle-tested across multiple systems and scenarios!*
