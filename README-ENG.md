# ğŸš€ Local-AI-project
### This is a local AI environment powered by Ollama, Docker, and Open-WebUI. It lets you run and interact with AI models (like LLaMA 3) right on your machine through a ChatGPT-like interfaceâ€”private, fast, and customizable in just 6 steps!


###  ğŸ¯ What You Get
###ğŸ›¡ï¸ Privacy-Focused AI: Keep your data localâ€”no cloud required.
        â†’ Your conversations and data stay on your machine, ensuring full control and privacy.

###  ğŸ§  Model Flexibility: Run and switch between models like LLaMA 2, Mistral, or any Ollama-supported model.
       â†’ Ollama supports a variety of open-source modelsâ€”pick the one that fits your needs.

###  ğŸ§© Personalization: Use Open-WebUI to give your AI memory and personality.
       â†’ Customize how your AI behaves and interacts with you.

###  ğŸ’¾ Persistent Storage: Docker volumes keep your models and data safeâ€”even after a reboot.
       â†’ Downloaded models and settings wonâ€™t disappear when the container stops or restarts.

###  ğŸŒ Optional Online Sharing: Use tools like ngrok to securely share your AI with others over the internet.
      â†’ Great for remote access, demos, or collaborating with friends and teams.




# ğŸ› ï¸ Installation Steps
Hereâ€™s how to set up your local AI environment from scratch. These are the exact steps I usedâ€”just follow along!


##1. ğŸ”§ Install Docker Desktop
  Open PowerShell or CMD as Administrator.
  
  Run the following command to install Docker:
    winget install --id Docker.DockerDesktop --source winget
  
 * Reboot your machine after installation. Launch Docker Desktop, complete the first-run setup, and make sure â€œUse the WSL 2 based engineâ€ is enabled. 



## 2. ğŸ“¦ Create a Persistent Volume for Ollama Models
  This makes sure your downloaded models survive container stops or deletions:  
    docker volume create ollama-data
   ![image](https://github.com/user-attachments/assets/8da63b15-09b4-48e2-8716-9ec9660330b7)



## 3. ğŸ¤– Spin Up Ollama as an HTTP Service
  This command runs Ollama and makes it available as an API:
    docker run -d --name ollama-server --gpus all -p 11434:11434 -v ollama-data:/root/.ollama -e OLLAMA_HOST="0.0.0.0:11434" ollama/ollama:latest serve
   
  ![image](https://github.com/user-attachments/assets/24ae70d9-86cd-4f16-aa0f-94c8993b39b2)


  ğŸ“˜ Breakdown:  
    -d: Run in background  
    --gpus all: Use your GPU if available  
    -p 11434:11434: Expose Ollamaâ€™s API port  
    -v ollama-data:/root/.ollama: Use your persistent volume  
    -e OLLAMA_HOST: Make Ollama listen for external connections  
    ğŸ‘‰ This turns Ollama into a model-serving service that tools like Open-WebUI can talk to.

    

## 4. ğŸ“¥ Pull a Model via Ollama CLI
  Download your preferred AI model (example below uses phi4-mini-reasoning):  
    docker exec -it ollama-server ollama pull phi4-mini-reasoning:latest
   
  ğŸ’¡ You can swap phi4-mini-reasoning:latest with any other modelâ€”like llama3.1:8b. Check out Ollamaâ€™s model library for more at https://ollama.com/search

## 5. ğŸŒ Spin Up Open-WebUI and Connect to Ollama
  First, create a volume for storing WebUIâ€™s data:
    docker volume create open-webui
  
  Then run the following command to launch the interface:
    docker run -d --name open-webui --restart always --gpus all -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data -e PROVIDERS="ollama" -e OLLAMA_URL="http://host.docker.internal:11434" ghcr.io/open-webui/open-webui:cuda

  
  ğŸ“˜ Breakdown:  
    --restart always: Auto-restarts on crash or reboot  
    --gpus all: Optional, for faster performance  
    -p 3000:8080: WebUI will be available at http://localhost:3000  
    -v open-webui:/app/backend/data: Persistent WebUI settings  
    -e PROVIDERS="ollama": Set Ollama as backend  
    -e OLLAMA_URL: Tell WebUI where to find your Ollama server



## 6. ğŸ—¨ï¸ Browse, Select, and Generate!
  Open your browser and go to:
  http://localhost:3000

![image](https://github.com/user-attachments/assets/ebc2a7e5-68c5-4e4f-9f1e-2d2e9fd0e0dd)

ğŸŒ Optional: Sharing Online with Ngrok
  Want to let friends or colleagues use your AI remotely? Use ngrok to create a secure tunnel to your localhost.  
  Sign up for a free account here: https://ngrok.com    
  ğŸ“ A full guide on using Ngrok is coming soon, but feel free to try it out on your own!



#â“ FAQs

  ## Q: I'm getting a "failed to write file: exit status 0xffffffff" error in Ubuntu WSL. Whatâ€™s the fix?
  A: This happens when Docker canâ€™t connect to your Ubuntu WSL distro. Here's how to fix it:
  
  Unregister the broken distro:
    wsl --unregister Ubuntu
  Re-install Ubuntu:
    wsl --install -d Ubuntu
  
  Go to Docker Desktop â†’ Settings â†’ Resources â†’ WSL Integration, and re-enable Ubuntu.
  
  Restart Docker Desktop.

## Q: Can I use different models?
  Absolutely! In step 4, replace phi4-mini-reasoning:latest with any other model like llama2:13b.
 ###  ğŸ‘‰ Check Ollamaâ€™s model library for more options!

## Q: How do I stop or delete the containers?
To stop the containers:
  docker stop ollama-server open-webui
To remove them:
  docker rm ollama-server open-webui
  
### ğŸ—‚ï¸ Note: Your data is safe in volumes, even if containers are deleted.

## Q: What if I donâ€™t have a GPU?
  No problem! Just remove the --gpus all flag from the Docker run commands.
  âš ï¸ Itâ€™ll run on your CPUâ€”slower, but it works!

# ğŸ‘ Credits
## Thanks to the amazing tools that made this project personal project possible:

### ğŸ’¡ Ollama
### ğŸ–¥ï¸ Open-WebUI
### ğŸ³ Docker
### ğŸŒ Ngrok

ğŸ’¬ Thatâ€™s it! You now have a full-featured, private, customizable, and extendable local AI environment. Enjoy your own ChatGPT-like assistantâ€”on your own terms!

ğŸ’¡I plan on the future show how to configurate and train the local Ai models using your own data! Follow along!
