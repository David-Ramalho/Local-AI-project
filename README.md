# ğŸš€ Build Your Own Local AI Assistant with Docker, Ollama & Open-WebUI

Set up your own **private ChatGPT-like assistant** locally using Ollama, Docker, and Open-WebUI.  
This guide walks you through a proven, no-nonsense setup in just 6 stepsâ€”tested on both Windows and Linux.

---

## ğŸ“š Table of Contents
- [What You Get](#-what-you-get)
- [What Each Tool Does](#-what-each-tool-does)
- [Installation Steps](#ï¸-installation-steps)
- [Optional: Sharing Online with Ngrok](#-optional-sharing-online-with-ngrok)
- [FAQs](#-faqs)
- [Credits](#-credits)
- [Coming Soon](#-coming-soon)

---

## ğŸ¯ What You Get

- ğŸ›¡ï¸ Privacy-Focused AI
     Keep your data localâ€”no cloud required. Your conversations and data stay on your machine, ensuring full control and privacy.

-ğŸ§  Model Flexibility  
   Run and switch between models like LLaMA 2, Mistral, or any Ollama-supported model.

- ğŸ§© Personalization  
   Use Open-WebUI to give your AI memory and personality. Customize how your AI behaves and interacts with you.

- ğŸ’¾ Persistent Storage  
   Docker volumes keep your models and data safeâ€”even after a reboot.

- ğŸŒ Optional Online Sharing  
   Use tools like ngrok to securely share your AI with others over the internet.

---

## ğŸ§° What Each Tool Does

- **Docker**: Runs all components in containersâ€”portable and isolated.
- **Ollama**: Hosts and serves your AI models locally.
- **Open-WebUI**: A sleek browser interface to chat with your models.
- **Ngrok (optional)**: Securely share your local AI assistant online.

---


# ğŸ› ï¸ Installation Steps   
   These are the exact steps I structured through multiple trial-and-error setups (over five full installs on both Windows plus on Linux). Just follow along!
   PS: This is Windows' based installation process 

## ğŸ–¥ï¸ Prerequisites & Tested On

Here are the system specs and versions used during testing. Other setups may work, but this is a proven baseline:

| Component         | Minimum Version / Notes                      | Tested On              |
|------------------|-----------------------------------------------|------------------------|
| Docker Desktop    | Latest stable  | Windows 11 (WSL2)           |  âœ…                    |
| Docker CLI        | Comes with Docker Desktop                    |  âœ…                    |
| Ollama            |  Latest stable                               |  âœ…                    |
| Open-WebUI        |  Latest stable                               |  âœ…                    |
| GPU               | NVIDIA GTX 1650 4GB or better                |  âœ…                    |
| RAM               | 16 GB minimum                                | 16 GB                   |
| CPU               | Intel i3 (9th Gen or better)                 | Intel i3-9100F          |


### âš™ï¸ Configuration

Before running the containers, here are the key environment variables used:

| Variable       | Description                          | Default / Example Value       |
|----------------|--------------------------------------|-------------------------------|
| `OLLAMA_HOST`  | Ollama API listening address         | `0.0.0.0:11434`               |
| `PROVIDERS`    | Specifies backend for Open-WebUI     | `ollama`                      |
| `OLLAMA_URL`   | URL to connect WebUI to Ollama       | `http://host.docker.internal:11434` |



## 1. ğŸ”§ Install Docker Desktop
   Open PowerShell or CMD as Administrator.           
    Run the following command to install Docker:
    
     winget install --id Docker.DockerDesktop --source winget
          
   * Reboot your machine after installation. Launch Docker Desktop, complete the first-run setup, and make sure â€œUse the WSL 2 based engineâ€ is enabled. 



## 2. ğŸ“¦ Create a Persistent Volume for Ollama Models
   This makes sure your downloaded models survive container stops or deletions:  
   
    docker volume create ollama-data
   ![image](https://github.com/user-attachments/assets/8da63b15-09b4-48e2-8716-9ec9660330b7)



## 3. ğŸ¤– Spin Up Ollama as an HTTP Service
   This command runs Ollama and makes it available as an API:
   
    docker run -d --name ollama-server --restart always --gpus all -p 11434:11434 -v ollama-data:/root/.ollama -e OLLAMA_HOST="0.0.0.0:11434" ollama/ollama:latest serve
           
   ![image](https://github.com/user-attachments/assets/24ae70d9-86cd-4f16-aa0f-94c8993b39b2)


   ğŸ“˜ Breakdown:  
   
            -d: Run in background  
            --gpus all: Use your GPU if available  
            -p 11434:11434: Expose Ollamaâ€™s API port  
            -v ollama-data:/root/.ollama: Use your persistent volume  
            -e OLLAMA_HOST: Make Ollama listen for external connections  
            ğŸ‘‰ This turns Ollama into a model-serving service that tools like Open-WebUI can talk to.

    

## 4. ğŸ“¥ Pull a Model via Ollama CLI
   Download your preferred AI model (example below uses phi4-mini-reasoning):  
   
     docker exec -it ollama-server ollama pull qwen3:1.7b
           
   ğŸ’¡ You can swap qwen3:1.7b with any other modelâ€”like llama3.1:8b. Check out Ollamaâ€™s model library for more at https://ollama.com/search

## 5. ğŸŒ Spin Up Open-WebUI and Connect to Ollama
   First, create a volume for storing WebUIâ€™s data:
   
    docker volume create open-webui
          
   Then run the following command to launch the interface:
   
    docker run -d --name open-webui --restart always --gpus all -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data -e PROVIDERS="ollama" -e OLLAMA_URL="http://host.docker.internal:11434" ghcr.io/open-webui/open-webui:cuda
        
  
   ğŸ“˜ Breakdown:  
   
            --restart always: Auto-restarts on crash or reboot  
            --gpus all: Optional, for faster performance  
            -p 3000:8080: WebUI will be available at http://localhost:3000  
            -v open-webui:/app/backend/data: Persistent WebUI settings  
            -e PROVIDERS="ollama": Set Ollama as backend  
            -e OLLAMA_URL: Tell WebUI where to find your Ollama server
        


## 6. ğŸ—¨ï¸ Browse, Select, and Generate!
   Open your browser and go to:
   
          http://localhost:3000
        
   ![image](https://github.com/user-attachments/assets/ebc2a7e5-68c5-4e4f-9f1e-2d2e9fd0e0dd)

## ğŸŒ Optional: Sharing Online with Ngrok
   Want to let friends or colleagues use your AI remotely? Use ngrok to create a secure tunnel to your localhost.  
    Sign up for a free account here: 
    
        https://ngrok.com 
        
   Create a tunnel to port 3000:
   
        ngrok http 3000

It gives you a temporary link to access your local app over the internet.
        
   ğŸ“ A full guide on using Ngrok is coming soon, but feel free to try it out on your own!


# â“ FAQs

   ## Q: I'm getting a "failed to write file: exit status 0xffffffff" error in Ubuntu WSL. Whatâ€™s the fix?
   
   A: This happens when Docker canâ€™t connect to your Ubuntu WSL distro. Here's how to fix it:
          
          Unregister the broken distro:
            wsl --unregister Ubuntu
          Re-install Ubuntu:
            wsl --install -d Ubuntu
          
   Go to Docker Desktop â†’ Settings â†’ Resources â†’ WSL Integration, and re-enable Ubuntu.
          
   Restart Docker Desktop.
        
   ## Q: Can I use different models?
   
   Absolutely! In step 4, replace phi4-mini-reasoning:latest with any other model like llama2:13b.
   
   ###  ğŸ‘‰ Check Ollamaâ€™s model library for more options!
        
   ## Q: How do I stop or delete the containers?
   To stop the containers:
   
          docker stop ollama-server open-webui
   To remove them:
   
          docker rm ollama-server open-webui
          
  ### ğŸ—‚ï¸ Note: Your data is safe in volumes, even if containers are deleted.
        
   ## Q: What if I donâ€™t have a GPU?
   
   No problem! Just remove the --gpus all flag from the Docker run commands.   
   âš ï¸ Itâ€™ll run on your CPUâ€”slower, but it works!

#  ğŸ‘ Credits
   ## Thanks to the amazing tools that made this project personal project possible:
        
      ğŸ’¡ Ollama

      ğŸ–¥ï¸ Open-WebUI
      
      ğŸ³ Docker
      
      ğŸŒ Ngrok

#  ğŸ”® Coming Soon
    How to configure memory and persistent conversations trough Rag files settings
    
    How to train models using your own documents via finetunin using unsloth(On going)  

    More optimization and customization tips!
    

ğŸ’¬ Thatâ€™s it! You now have a full-featured, private, customizable, and extendable local AI environment. Enjoy your own ChatGPT-like assistantâ€”on your own terms!
